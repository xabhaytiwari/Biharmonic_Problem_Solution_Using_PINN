{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xabhaytiwari/Biharmonic_Problem_Solution_Using_PINN/blob/main/Biharmonic_Problem_Solution_Using_PINN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0p7Z-xM1K8S2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class NN(torch.nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super(NN, self).__init__()\n",
        "    self.L1 = torch.nn.Linear(2, 60) # Wider\n",
        "    self.L2 = torch.nn.Linear(60, 60)\n",
        "    self.L3 = torch.nn.Linear(60, 60)\n",
        "    self.L4 = torch.nn.Linear(60, 60)\n",
        "    self.L5 = torch.nn.Linear(60, 60) # Deeper\n",
        "    self.L6 = torch.nn.Linear(60, 1)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    inputs = torch.cat([x, y], axis=1)\n",
        "    x1 = torch.tanh(self.L1(inputs))\n",
        "    x2 = torch.tanh(self.L2(x1))\n",
        "    x3 = torch.tanh(self.L3(x2))\n",
        "    x4 = torch.tanh(self.L4(x3))\n",
        "    x5 = torch.tanh(self.L5(x4))\n",
        "    x6 = self.L6(x5)\n",
        "    return x6\n",
        "\n",
        "  def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll gather Inputs, Collocation points, PDE Data and Balancing Parameter alpha.\n"
      ],
      "metadata": {
        "id": "Ks7LqF3FM7PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff, sin, pi, lambdify\n",
        "\n",
        "x1, x2 = symbols('x1 x2')\n",
        "u = (1 / (2 * (pi ** 2))) * sin(pi * x1) * sin(pi * x2)\n",
        "# u = (x1 ** 2) * (x2 ** 2) * ((1 - x1) ** 2) * ((1 - x2) ** 2)\n",
        "\n",
        "laplacian_u = diff(u, x1, 2) + diff(u, x2, 2)\n",
        "bi_laplacian_u = diff(laplacian_u, x1, 2) + diff(laplacian_u, x2, 2)\n",
        "\n",
        "f_sym = bi_laplacian_u\n",
        "\n",
        "u_x_sym = diff(u, x1)\n",
        "u_y_sym = diff(u, x2)\n",
        "\n",
        "u_xx_sym = diff(u, x1, 2)\n",
        "u_yy_sym = diff(u, x2, 2)\n",
        "u_xy_sym = diff(u, x1, x2)\n",
        "\n",
        "ldu = lambdify((x1, x2), u, 'numpy')\n",
        "ldu_x = lambdify((x1, x2), u_x_sym, 'numpy')\n",
        "ldu_y = lambdify((x1, x2), u_y_sym, 'numpy')\n",
        "ldf = lambdify((x1, x2), f_sym, 'numpy')\n",
        "\n",
        "ldu_xx = lambdify((x1, x2), u_xx_sym, 'numpy')\n",
        "ldu_yy = lambdify((x1, x2), u_yy_sym, 'numpy')\n",
        "ldu_xy = lambdify((x1, x2), u_xy_sym, 'numpy')\n",
        "\n",
        "def from_seq_to_array(items):\n",
        "    out = []\n",
        "    for item in items:\n",
        "        out.append(np.array(item).reshape(-1, 1))\n",
        "    if len(out) == 1:\n",
        "        return out[0]\n",
        "    return out\n",
        "\n",
        "\n",
        "def data_gen_interior(collocations):\n",
        "    u_gt = [ldu(x, y) for x, y in collocations]\n",
        "    f_gt = [ldf(x, y) for x, y in collocations]\n",
        "    return from_seq_to_array([u_gt, f_gt])\n",
        "\n",
        "def data_gen_bdry(collocations, normal_vec):\n",
        "    ybdry_vals_g1       = []\n",
        "    ybdry_vals_g2       = []\n",
        "\n",
        "    for (x, y), (n1, n2) in zip(collocations, normal_vec):\n",
        "        ybdry_vals_g1.append(ldu(x, y))\n",
        "        ybdry_vals_g2.append(ldu_x(x, y) * n1 + ldu_y(x, y) * n2)\n",
        "\n",
        "    return from_seq_to_array([ybdry_vals_g1, ybdry_vals_g2])"
      ],
      "metadata": {
        "id": "K3xFxKlXNmDk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from scipy.stats import uniform\n",
        "import os\n",
        "\n",
        "N = 20000\n",
        "dataname = '20000pts'\n",
        "\n",
        "domain_data_x = uniform.rvs(size=N)\n",
        "domain_data_y = uniform.rvs(size=N)\n",
        "\n",
        "domain_data = np.array([domain_data_x,domain_data_y]).T\n",
        "print(domain_data.shape)\n",
        "\n",
        "\n",
        "Nb = 10000\n",
        "\n",
        "def generate_random_bdry(Nb):\n",
        "    '''\n",
        "    Generate random boundary points.\n",
        "    '''\n",
        "    bdry_col = uniform.rvs(size=Nb*2).reshape([Nb,2])\n",
        "    for i in range(Nb):\n",
        "        randind = np.random.randint(0,2)\n",
        "        if bdry_col[i,randind] <= 0.5:\n",
        "            bdry_col[i,randind] = 0.0\n",
        "        else:\n",
        "            bdry_col[i,randind] = 1.0\n",
        "\n",
        "    return bdry_col\n",
        "\n",
        "\n",
        "def compute_normals(bdry_col, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Given bdry_col of shape (Nb,2) with points on the edges of [0,1]^2,\n",
        "    returns two arrays n1,n2 of shape (Nb,1) giving the outward unit normals.\n",
        "\n",
        "    Assumes:\n",
        "      - if x ≈ 0 → normal = (-1, 0)\n",
        "      - if x ≈ 1 → normal = ( 1, 0)\n",
        "      - if y ≈ 0 → normal = ( 0,-1)\n",
        "      - if y ≈ 1 → normal = ( 0, 1)\n",
        "    \"\"\"\n",
        "    x = bdry_col[:, 0]\n",
        "    y = bdry_col[:, 1]\n",
        "\n",
        "    n1 = np.zeros_like(x)\n",
        "    n2 = np.zeros_like(y)\n",
        "\n",
        "    # left edge x=0\n",
        "    mask = np.isclose(x, 0.0, atol=eps)\n",
        "    n1[mask] = -1.0;  n2[mask] =  0.0\n",
        "\n",
        "    # right edge x=1\n",
        "    mask = np.isclose(x, 1.0, atol=eps)\n",
        "    n1[mask] =  1.0;  n2[mask] =  0.0\n",
        "\n",
        "    # bottom edge y=0\n",
        "    mask = np.isclose(y, 0.0, atol=eps)\n",
        "    n1[mask] =  0.0;  n2[mask] = -1.0\n",
        "\n",
        "    # top edge y=1\n",
        "    mask = np.isclose(y, 1.0, atol=eps)\n",
        "    n1[mask] =  0.0;  n2[mask] =  1.0\n",
        "\n",
        "    # reshape to column vectors\n",
        "    return n1.reshape(-1,1), n2.reshape(-1,1)\n",
        "\n",
        "bdry_col = generate_random_bdry(Nb)\n",
        "n1_np, n2_np = compute_normals(bdry_col)\n",
        "normal_vec = np.hstack([n1_np, n2_np])\n",
        "\n",
        "print(normal_vec.shape)\n",
        "print(bdry_col.shape)\n",
        "\n",
        "if not os.path.exists('dataset/'):\n",
        "    os.makedirs('dataset/')\n",
        "with open('dataset/'+dataname,'wb') as pfile:\n",
        "    pkl.dump(domain_data,pfile)\n",
        "    pkl.dump(bdry_col,pfile)\n",
        "    pkl.dump(normal_vec,pfile)\n",
        "\n",
        "\n",
        "ugt,fgt = data_gen_interior(domain_data)\n",
        "dirichlet_data, clamped_data = data_gen_bdry(bdry_col,normal_vec)\n",
        "\n",
        "\n",
        "with open(\"dataset/gt_on_{}\".format(dataname),'wb') as pfile:\n",
        "    pkl.dump(ugt,pfile)\n",
        "    pkl.dump(fgt,pfile)\n",
        "    pkl.dump(dirichlet_data,pfile)\n",
        "    pkl.dump(clamped_data,pfile)"
      ],
      "metadata": {
        "id": "_C8FJwO4NYEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9855f24-631e-48f4-a052-ddcdd93e8f08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 2)\n",
            "(10000, 2)\n",
            "(10000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "def pde(x1, x2, net):\n",
        "    \"\"\"\n",
        "    Compute Δ²u = (u_xx + u_yy)_xx + (u_xx + u_yy)_yy\n",
        "    at the points (x1, x2) via automatic differentiation.\n",
        "    \"\"\"\n",
        "    # 1) forward pass\n",
        "    u = net(x1, x2)\n",
        "\n",
        "    # 2) first derivatives\n",
        "    # u_x, u_y = torch.autograd.grad(\n",
        "    #     u.sum(), (x1, x2), create_graph=True\n",
        "    # )\n",
        "    u_x = torch.autograd.grad(u.sum(), x1, create_graph=True)[0]\n",
        "    u_y = torch.autograd.grad(u.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "    # 3) second derivatives\n",
        "    u_xx = torch.autograd.grad(u_x.sum(), x1, create_graph=True)[0]\n",
        "    u_yy = torch.autograd.grad(u_y.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "    # 4) Laplacian\n",
        "    lap = u_xx + u_yy\n",
        "\n",
        "    # 5) third derivatives of lap\n",
        "    # lap_x, lap_y = torch.autograd.grad(\n",
        "    #     lap.sum(), (x1, x2), create_graph=True\n",
        "    # )\n",
        "    lap_x = torch.autograd.grad(lap.sum(), x1, create_graph=True)[0]\n",
        "    lap_y = torch.autograd.grad(lap.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "    # 6) fourth derivatives (bi‐Laplacian pieces)\n",
        "    lap_xx = torch.autograd.grad(lap_x.sum(), x1, create_graph=True)[0]\n",
        "    lap_yy = torch.autograd.grad(lap_y.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "\n",
        "\n",
        "    return lap_xx +  lap_yy\n",
        "\n",
        "\n",
        "\n",
        "def bdry(x1,x2,n1,n2,net):\n",
        "    out = net(x1,x2)\n",
        "    u_x = torch.autograd.grad(out.sum(),x1,create_graph=True)[0]\n",
        "    u_y = torch.autograd.grad(out.sum(),x2,create_graph=True)[0]\n",
        "\n",
        "    return out, u_x*n1 + u_y*n2\n",
        "\n",
        "\n",
        "def pdeloss(net,intx1,intx2,pdedata,bdx1,bdx2,nx1,nx2,bdrydata_diri,bdrydat_clamped,bw_diri,bw_clamped):\n",
        "    bdx1  = bdx1.requires_grad_(True)\n",
        "    bdx2  = bdx2.requires_grad_(True)\n",
        "    pout = pde(intx1,intx2,net)\n",
        "    bout_diri, bout_clamped = bdry(bdx1,bdx2,nx1,nx2,net)\n",
        "    pres = mse_loss(pout,pdedata)\n",
        "    bres = bw_diri*mse_loss(bout_diri,bdrydata_diri) + bw_clamped*mse_loss(bout_clamped,bdrydat_clamped)\n",
        "    diri_loss = mse_loss(bout_diri,bdrydata_diri)\n",
        "    clamped_loss = mse_loss(bout_clamped,bdrydat_clamped)\n",
        "\n",
        "\n",
        "    loss = pres + bres\n",
        "\n",
        "    return loss, pres, bres, diri_loss, clamped_loss"
      ],
      "metadata": {
        "id": "kogMf2FV4btP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def from_numpy_to_tensor(numpys,require_grads,dtype=torch.float32):\n",
        "    outputs = list()\n",
        "    for ind in range(len(numpys)):\n",
        "        outputs.append(\n",
        "            Variable(torch.from_numpy(numpys[ind]),requires_grad=require_grads[ind]).type(dtype)\n",
        "        )\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "uwukoYQM8Vob"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from tracemalloc import start\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.optim as opt\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.utils.data.dataloader as Dataloader\n",
        "from torch.autograd import Variable\n",
        "import pickle as pkl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "from time import time\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "torch.set_default_dtype(torch.float32)\n",
        "start_time = time()\n",
        "\n",
        "y = NN()\n",
        "y.apply(NN.init_weights)\n",
        "y.to(device)\n",
        "\n",
        "dataname = '20000pts'\n",
        "name = 'results/'\n",
        "bw_dir = 1e4   # Start with 10,000\n",
        "bw_clamp = 1e4 # Start with 10,000\n",
        "\n",
        "if not os.path.exists(name):\n",
        "    os.makedirs(name)\n",
        "if not os.path.exists(name+\"y_plot/\"):\n",
        "    os.makedirs(name+\"y_plot/\")\n",
        "\n",
        "params = list(y.parameters())\n",
        "\n",
        "# --- 1. Load all data from files ---\n",
        "with open(\"dataset/\"+dataname,'rb') as pfile:\n",
        "    int_col = pkl.load(pfile)\n",
        "    bdry_col = pkl.load(pfile)\n",
        "    normal_vec = pkl.load(pfile)\n",
        "\n",
        "with open(\"dataset/gt_on_{}\".format(dataname),'rb') as pfile:\n",
        "    u_gt_np = pkl.load(pfile)\n",
        "    f_np = pkl.load(pfile)\n",
        "    dirichlet_data_np = pkl.load(pfile)\n",
        "    clamped_data_np = pkl.load(pfile)\n",
        "\n",
        "print(f\"Interior points: {int_col.shape}\")\n",
        "print(f\"Boundary points: {bdry_col.shape}\")\n",
        "print(f\"Forcing term: {f_np.shape}\")\n",
        "print(f\"Dirichlet data: {dirichlet_data_np.shape}\")\n",
        "print(f\"Clamped data: {clamped_data_np.shape}\")\n",
        "\n",
        "def compute_derivatives(net, x, y):\n",
        "    \"\"\" Computes u, 1st, and 2nd derivatives \"\"\"\n",
        "    u = net(x, y)\n",
        "\n",
        "    # 1st derivatives\n",
        "    u_x, u_y = torch.autograd.grad(\n",
        "        u.sum(), (x, y), create_graph=True\n",
        "    )\n",
        "\n",
        "    # 2nd derivatives\n",
        "    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
        "    u_yy = torch.autograd.grad(u_y.sum(), y, create_graph=True)[0]\n",
        "    u_xy = torch.autograd.grad(u_x.sum(), y, create_graph=True)[0]\n",
        "\n",
        "    return u, u_x, u_y, u_xx, u_yy, u_xy\n",
        "\n",
        "# --- 2. Create Tensors and DataLoaders ---\n",
        "\n",
        "# Create a TensorDataset for INTERIOR points AND their forcing term ---\n",
        "intx1_np, intx2_np = np.split(int_col, 2, axis=1)\n",
        "\n",
        "# Convert interior data to tensors (no grad needed here, will be set in loop)\n",
        "tintx1 = torch.from_numpy(intx1_np).float()\n",
        "tintx2 = torch.from_numpy(intx2_np).float()\n",
        "f_tensor = torch.from_numpy(f_np).float()\n",
        "\n",
        "# Create a dataset and loader for interior points\n",
        "interior_dataset = TensorDataset(tintx1, tintx2, f_tensor)\n",
        "loader = torch.utils.data.DataLoader(interior_dataset, batch_size=2000, shuffle=True)\n",
        "\n",
        "\n",
        "# Create Tensors for BOUNDARY data (with requires_grad=True) ---\n",
        "bdx1_np, bdx2_np = np.split(bdry_col, 2, axis=1)\n",
        "nx1_np, nx2_np = np.split(normal_vec, 2, axis=1)\n",
        "\n",
        "# Boundary coordinates MUST have requires_grad=True\n",
        "# Normals and ground truth data do NOT need gradients.\n",
        "[tbdx1, tbdx2] = from_numpy_to_tensor([bdx1_np, bdx2_np], [True, True], dtype=torch.float32)\n",
        "[tnx1, tnx2] = from_numpy_to_tensor([nx1_np, nx2_np], [False, False], dtype=torch.float32)\n",
        "[bdrydata_dirichlet, bdrydata_clamped] = from_numpy_to_tensor(\n",
        "    [dirichlet_data_np, clamped_data_np], [False, False], dtype=torch.float32\n",
        ")\n",
        "\n",
        "# Move all *full* boundary tensors to the device once\n",
        "tbdx1 = tbdx1.to(device)\n",
        "tbdx2 = tbdx2.to(device)\n",
        "tnx1 = tnx1.to(device)\n",
        "tnx2 = tnx2.to(device)\n",
        "bdrydata_dirichlet = bdrydata_dirichlet.to(device)\n",
        "bdrydata_clamped = bdrydata_clamped.to(device)\n",
        "\n",
        "# Note: We won't use ygt for training, only for final error calculation\n",
        "ygt = torch.from_numpy(u_gt_np).float().to(device)\n",
        "\n",
        "optimizer = opt.Adam(params, lr=1e-4)\n",
        "scheduler = opt.lr_scheduler.ReduceLROnPlateau(optimizer, patience=500)\n",
        "train_start_time = time()\n",
        "# --- 3. Define the Closure ---\n",
        "def closure():\n",
        "    tot_loss = 0\n",
        "    tot_pres = 0\n",
        "    tot_diri_loss = 0\n",
        "    tot_clamped_loss = 0\n",
        "\n",
        "    # Iterate over the DataLoader\n",
        "    for i, (intx1_batch, intx2_batch, f_batch) in enumerate(loader):\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       # Set requires_grad=True for the BATCH coordinates\n",
        "       ttintx1 = intx1_batch.to(device).requires_grad_(True)\n",
        "       ttintx2 = intx2_batch.to(device).requires_grad_(True)\n",
        "\n",
        "       # This is the ground-truth forcing term for the batch\n",
        "       f_gt_batch = f_batch.to(device)\n",
        "\n",
        "       # --- Pass the correct batch data to the loss function ---\n",
        "       # We use the *batched* interior points and *batched* forcing term\n",
        "       # We use the *full* set of boundary points\n",
        "       loss, pres, bres, diri_loss, clamped_loss = pdeloss(\n",
        "           y,\n",
        "           ttintx1, ttintx2, f_gt_batch,  # Batched interior data\n",
        "           tbdx1, tbdx2, tnx1, tnx2,     # Full boundary data\n",
        "           bdrydata_dirichlet, bdrydata_clamped,\n",
        "           bw_dir, bw_clamp\n",
        "       )\n",
        "\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       # Detach losses to free memory\n",
        "       tot_loss += loss.detach()\n",
        "       tot_pres += pres.detach()\n",
        "       tot_diri_loss += diri_loss.detach()\n",
        "       tot_clamped_loss += clamped_loss.detach()\n",
        "\n",
        "    # Get the average loss per batch\n",
        "    num_batches = len(loader)\n",
        "    avg_loss = tot_loss / num_batches\n",
        "    avg_pres = tot_pres / num_batches\n",
        "    avg_diri = tot_diri_loss / num_batches\n",
        "    avg_clamp = tot_clamped_loss / num_batches\n",
        "\n",
        "    # scheduler.step() uses the average loss\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    return (\n",
        "        avg_loss.cpu().numpy(),\n",
        "        avg_pres.cpu().numpy(),\n",
        "        avg_diri.cpu().numpy(),\n",
        "        avg_clamp.cpu().numpy()\n",
        "    )\n",
        "\n",
        "# --- 4. Run the Training Loop ---\n",
        "losslist = list()\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(20000):\n",
        "    loss, presloss, diri_loss, clamped_loss = closure()\n",
        "    losslist.append(loss)\n",
        "    if epoch % 100 == 0:\n",
        "         print(f\"Epoch {epoch} | Total loss: {loss:.8f} | Loss_int: {presloss:.8f} | Loss_Clamped: {clamped_loss:.8f} | Loss_dirichlet: {diri_loss:.8f}\")\n",
        "train_end_time = time()\n",
        "print(\"Training finished.\")\n",
        "with open(\"results/loss.pkl\",'wb') as pfile:\n",
        "    pkl.dump(losslist,pfile)\n",
        "\n",
        "# --- (After training is finished) ---\n",
        "\n",
        "print(\"Calculating final errors...\")\n",
        "\n",
        "# Use the same grid as your plots\n",
        "x_pts = np.linspace(0,1,200)\n",
        "y_pts = np.linspace(0,1,200)\n",
        "ms_x, ms_y = np.meshgrid(x_pts,y_pts)\n",
        "x_flat = np.ravel(ms_x).reshape(-1,1)\n",
        "y_flat = np.ravel(ms_y).reshape(-1,1)\n",
        "collocations = np.concatenate([x_flat, y_flat], axis=1)\n",
        "\n",
        "# --- 1. Get Ground Truth (GT) values ---\n",
        "u_gt_np = ldu(x_flat, y_flat).flatten()\n",
        "ux_gt_np = ldu_x(x_flat, y_flat).flatten()\n",
        "uy_gt_np = ldu_y(x_flat, y_flat).flatten()\n",
        "uxx_gt_np = ldu_xx(x_flat, y_flat).flatten()\n",
        "uyy_gt_np = ldu_yy(x_flat, y_flat).flatten()\n",
        "uxy_gt_np = ldu_xy(x_flat, y_flat).flatten()\n",
        "\n",
        "# --- 2. Get Network Prediction (NN) values ---\n",
        "pt_x = torch.from_numpy(x_flat).float().to(device).requires_grad_(True)\n",
        "pt_y = torch.from_numpy(y_flat).float().to(device).requires_grad_(True)\n",
        "\n",
        "u_nn, ux_nn, uy_nn, uxx_nn, uyy_nn, uxy_nn = compute_derivatives(y, pt_x, pt_y)\n",
        "\n",
        "u_nn_np = u_nn.data.cpu().numpy().flatten()\n",
        "ux_nn_np = ux_nn.data.cpu().numpy().flatten()\n",
        "uy_nn_np = uy_nn.data.cpu().numpy().flatten()\n",
        "uxx_nn_np = uxx_nn.data.cpu().numpy().flatten()\n",
        "uyy_nn_np = uyy_nn.data.cpu().numpy().flatten()\n",
        "uxy_nn_np = uxy_nn.data.cpu().numpy().flatten()\n",
        "\n",
        "# --- 3. Compute Errors ---\n",
        "# L2 Error\n",
        "l2_error_u = np.sqrt(np.mean((u_gt_np - u_nn_np)**2))\n",
        "l2_error = l2_error_u / np.sqrt(np.mean(u_gt_np**2))\n",
        "\n",
        "# H1 Error\n",
        "l2_error_ux = np.sqrt(np.mean((ux_gt_np - ux_nn_np)**2))\n",
        "l2_error_uy = np.sqrt(np.mean((uy_gt_np - uy_nn_np)**2))\n",
        "h1_error = np.sqrt(l2_error_u**2 + l2_error_ux**2 + l2_error_uy**2) / \\\n",
        "           np.sqrt(np.mean(u_gt_np**2) + np.mean(ux_gt_np**2) + np.mean(uy_gt_np**2))\n",
        "\n",
        "# H2 Error\n",
        "l2_error_uxx = np.sqrt(np.mean((uxx_gt_np - uxx_nn_np)**2))\n",
        "l2_error_uyy = np.sqrt(np.mean((uyy_gt_np - uyy_nn_np)**2))\n",
        "l2_error_uxy = np.sqrt(np.mean((uxy_gt_np - uxy_nn_np)**2))\n",
        "h2_error = np.sqrt(l2_error_u**2 + l2_error_ux**2 + l2_error_uy**2 + \\\n",
        "                   l2_error_uxx**2 + l2_error_uyy**2 + l2_error_uxy**2) / \\\n",
        "           np.sqrt(np.mean(u_gt_np**2) + np.mean(ux_gt_np**2) + np.mean(uy_gt_np**2) + \\\n",
        "                   np.mean(uxx_gt_np**2) + np.mean(uyy_gt_np**2) + np.mean(uxy_gt_np**2))\n",
        "\n",
        "print(f\"Relative L2 Error: {l2_error:.4e}\")\n",
        "print(f\"Relative H1 Error: {h1_error:.4e}\")\n",
        "print(f\"Relative H2 Error: {h2_error:.4e}\")\n",
        "\n",
        "# --- 4. Reshape for Plotting ---\n",
        "ms_ysol = u_nn_np.reshape(ms_x.shape)\n",
        "ms_ugt = u_gt_np.reshape(ms_x.shape)\n",
        "\n",
        "# --- Plot Loss History ---\n",
        "print(\"Plotting loss history...\")\n",
        "fig_loss = plt.figure(figsize=(10, 6))\n",
        "plt.plot(losslist)\n",
        "plt.yscale('log') # Log scale is essential for loss plots\n",
        "plt.title('Training Loss Dynamics')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.savefig(name + 'loss_history.png')\n",
        "plt.close(fig_loss)\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "print(\"\\n---Training Summary ---\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"\\n[Network Architecture]\")\n",
        "print(y)\n",
        "print(\"\\n[Hyperparameters]\")\n",
        "print(f\"  Optimizer: Adam (lr={1e-4})\")\n",
        "print(f\"  Epochs (Adam): {20000}\")\n",
        "print(f\"  Batch Size: {2000}\")\n",
        "print(f\"  Interior Points: {int_col.shape[0]}\")\n",
        "print(f\"  Boundary Points: {bdry_col.shape[0]}\")\n",
        "print(f\"  Loss Weight (Dirichlet): {bw_dir:.1e}\")\n",
        "print(f\"  Loss Weight (Clamped): {bw_clamp:.1e}\")\n",
        "print(\"\\n[Performance]\")\n",
        "print(f\"  Training Time: {train_end_time - train_start_time:.2f} seconds\")\n",
        "print(f\"  Total Script Time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"  Final Total Loss: {losslist[-1]:.4e}\")\n",
        "print(f\"  Relative L2 Error: {l2_error:.4e}\")\n",
        "print(f\"  Relative H1 Error: {h1_error:.4e}\")\n",
        "print(f\"  Relative H2 Error: {h2_error:.4e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VNK2GLV70bN",
        "outputId": "6dbc1743-2870-404f-cc33-557818520800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Interior points: (20000, 2)\n",
            "Boundary points: (10000, 2)\n",
            "Forcing term: (20000, 1)\n",
            "Dirichlet data: (10000, 1)\n",
            "Clamped data: (10000, 1)\n",
            "Starting training...\n",
            "Epoch 0 | Total loss: 252.29716492 | Loss_int: 101.15697479 | Loss_Clamped: 0.01471752 | Loss_dirichlet: 0.00039650\n",
            "Epoch 100 | Total loss: 4.90596151 | Loss_int: 3.25206542 | Loss_Clamped: 0.00012578 | Loss_dirichlet: 0.00003961\n",
            "Epoch 200 | Total loss: 0.09467290 | Loss_int: 0.07720191 | Loss_Clamped: 0.00000111 | Loss_dirichlet: 0.00000063\n",
            "Epoch 300 | Total loss: 0.07795068 | Loss_int: 0.05726565 | Loss_Clamped: 0.00000111 | Loss_dirichlet: 0.00000096\n",
            "Epoch 400 | Total loss: 0.05302797 | Loss_int: 0.04683167 | Loss_Clamped: 0.00000026 | Loss_dirichlet: 0.00000036\n",
            "Epoch 500 | Total loss: 0.04731789 | Loss_int: 0.03941247 | Loss_Clamped: 0.00000030 | Loss_dirichlet: 0.00000049\n",
            "Epoch 600 | Total loss: 0.03787919 | Loss_int: 0.03337349 | Loss_Clamped: 0.00000013 | Loss_dirichlet: 0.00000032\n",
            "Epoch 700 | Total loss: 0.04329149 | Loss_int: 0.02846321 | Loss_Clamped: 0.00000058 | Loss_dirichlet: 0.00000090\n",
            "Epoch 800 | Total loss: 0.04155645 | Loss_int: 0.02461279 | Loss_Clamped: 0.00000092 | Loss_dirichlet: 0.00000078\n",
            "Epoch 900 | Total loss: 0.02979398 | Loss_int: 0.02143299 | Loss_Clamped: 0.00000032 | Loss_dirichlet: 0.00000052\n",
            "Epoch 1000 | Total loss: 0.02275454 | Loss_int: 0.01883933 | Loss_Clamped: 0.00000011 | Loss_dirichlet: 0.00000028\n",
            "Epoch 1100 | Total loss: 0.02409359 | Loss_int: 0.01672057 | Loss_Clamped: 0.00000026 | Loss_dirichlet: 0.00000048\n",
            "Epoch 1200 | Total loss: 0.03231012 | Loss_int: 0.01494415 | Loss_Clamped: 0.00000072 | Loss_dirichlet: 0.00000102\n",
            "Epoch 1300 | Total loss: 0.01807816 | Loss_int: 0.01345336 | Loss_Clamped: 0.00000015 | Loss_dirichlet: 0.00000031\n",
            "Epoch 1400 | Total loss: 0.01583271 | Loss_int: 0.01222165 | Loss_Clamped: 0.00000010 | Loss_dirichlet: 0.00000026\n",
            "Epoch 1500 | Total loss: 0.01832198 | Loss_int: 0.01120051 | Loss_Clamped: 0.00000026 | Loss_dirichlet: 0.00000045\n",
            "Epoch 1600 | Total loss: 0.02104006 | Loss_int: 0.01032644 | Loss_Clamped: 0.00000042 | Loss_dirichlet: 0.00000065\n",
            "Epoch 1700 | Total loss: 0.03149616 | Loss_int: 0.00955662 | Loss_Clamped: 0.00000098 | Loss_dirichlet: 0.00000121\n",
            "Epoch 1800 | Total loss: 0.01860182 | Loss_int: 0.00885798 | Loss_Clamped: 0.00000038 | Loss_dirichlet: 0.00000059\n",
            "Epoch 1900 | Total loss: 0.01769926 | Loss_int: 0.00825617 | Loss_Clamped: 0.00000038 | Loss_dirichlet: 0.00000057\n",
            "Epoch 2000 | Total loss: 0.04753752 | Loss_int: 0.00778398 | Loss_Clamped: 0.00000270 | Loss_dirichlet: 0.00000128\n",
            "Epoch 2100 | Total loss: 0.03975817 | Loss_int: 0.00729050 | Loss_Clamped: 0.00000143 | Loss_dirichlet: 0.00000182\n",
            "Epoch 2200 | Total loss: 0.03816339 | Loss_int: 0.00683168 | Loss_Clamped: 0.00000136 | Loss_dirichlet: 0.00000177\n",
            "Epoch 2300 | Total loss: 0.00894650 | Loss_int: 0.00638959 | Loss_Clamped: 0.00000007 | Loss_dirichlet: 0.00000019\n",
            "Epoch 2400 | Total loss: 0.00901971 | Loss_int: 0.00601219 | Loss_Clamped: 0.00000009 | Loss_dirichlet: 0.00000021\n",
            "Epoch 2500 | Total loss: 0.01553829 | Loss_int: 0.00569485 | Loss_Clamped: 0.00000040 | Loss_dirichlet: 0.00000058\n",
            "Epoch 2600 | Total loss: 0.10995356 | Loss_int: 0.00560608 | Loss_Clamped: 0.00000468 | Loss_dirichlet: 0.00000576\n",
            "Epoch 2700 | Total loss: 0.00773263 | Loss_int: 0.00510104 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000019\n",
            "Epoch 2800 | Total loss: 0.01071183 | Loss_int: 0.00485996 | Loss_Clamped: 0.00000024 | Loss_dirichlet: 0.00000034\n",
            "Epoch 2900 | Total loss: 0.02118232 | Loss_int: 0.00464899 | Loss_Clamped: 0.00000080 | Loss_dirichlet: 0.00000085\n",
            "Epoch 3000 | Total loss: 0.00691809 | Loss_int: 0.00440664 | Loss_Clamped: 0.00000007 | Loss_dirichlet: 0.00000018\n",
            "Epoch 3100 | Total loss: 0.00664659 | Loss_int: 0.00421538 | Loss_Clamped: 0.00000007 | Loss_dirichlet: 0.00000017\n",
            "Epoch 3200 | Total loss: 0.01297362 | Loss_int: 0.00405371 | Loss_Clamped: 0.00000037 | Loss_dirichlet: 0.00000052\n",
            "Epoch 3300 | Total loss: 0.00611042 | Loss_int: 0.00387380 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000016\n",
            "Epoch 3400 | Total loss: 0.00597335 | Loss_int: 0.00372468 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000016\n",
            "Epoch 3500 | Total loss: 0.00573880 | Loss_int: 0.00356846 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000016\n",
            "Epoch 3600 | Total loss: 0.00582613 | Loss_int: 0.00343887 | Loss_Clamped: 0.00000007 | Loss_dirichlet: 0.00000017\n",
            "Epoch 3700 | Total loss: 0.00548881 | Loss_int: 0.00330820 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000015\n",
            "Epoch 3800 | Total loss: 0.00536518 | Loss_int: 0.00319173 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000015\n",
            "Epoch 3900 | Total loss: 0.01733604 | Loss_int: 0.00311036 | Loss_Clamped: 0.00000062 | Loss_dirichlet: 0.00000080\n",
            "Epoch 4000 | Total loss: 0.00997640 | Loss_int: 0.00298849 | Loss_Clamped: 0.00000028 | Loss_dirichlet: 0.00000042\n",
            "Epoch 4100 | Total loss: 0.00557418 | Loss_int: 0.00288372 | Loss_Clamped: 0.00000009 | Loss_dirichlet: 0.00000018\n",
            "Epoch 4200 | Total loss: 0.01727308 | Loss_int: 0.00282070 | Loss_Clamped: 0.00000093 | Loss_dirichlet: 0.00000051\n",
            "Epoch 4300 | Total loss: 0.00606126 | Loss_int: 0.00269318 | Loss_Clamped: 0.00000016 | Loss_dirichlet: 0.00000018\n",
            "Epoch 4400 | Total loss: 0.00797678 | Loss_int: 0.00262378 | Loss_Clamped: 0.00000027 | Loss_dirichlet: 0.00000026\n",
            "Epoch 4500 | Total loss: 0.00442687 | Loss_int: 0.00253117 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000014\n",
            "Epoch 4600 | Total loss: 0.02508461 | Loss_int: 0.00251442 | Loss_Clamped: 0.00000133 | Loss_dirichlet: 0.00000092\n",
            "Epoch 4700 | Total loss: 0.04457990 | Loss_int: 0.00247676 | Loss_Clamped: 0.00000289 | Loss_dirichlet: 0.00000132\n",
            "Epoch 4800 | Total loss: 0.01712337 | Loss_int: 0.00233599 | Loss_Clamped: 0.00000065 | Loss_dirichlet: 0.00000083\n",
            "Epoch 4900 | Total loss: 0.00403115 | Loss_int: 0.00224218 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000013\n",
            "Epoch 5000 | Total loss: 0.03886552 | Loss_int: 0.00224351 | Loss_Clamped: 0.00000162 | Loss_dirichlet: 0.00000204\n",
            "Epoch 5100 | Total loss: 0.06388741 | Loss_int: 0.00227741 | Loss_Clamped: 0.00000414 | Loss_dirichlet: 0.00000202\n",
            "Epoch 5200 | Total loss: 0.00494349 | Loss_int: 0.00206972 | Loss_Clamped: 0.00000010 | Loss_dirichlet: 0.00000019\n",
            "Epoch 5300 | Total loss: 0.00371004 | Loss_int: 0.00201163 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000012\n",
            "Epoch 5400 | Total loss: 0.00386801 | Loss_int: 0.00195152 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000013\n",
            "Epoch 5500 | Total loss: 0.00356554 | Loss_int: 0.00190486 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000012\n",
            "Epoch 5600 | Total loss: 0.12370324 | Loss_int: 0.00211538 | Loss_Clamped: 0.00000546 | Loss_dirichlet: 0.00000670\n",
            "Epoch 5700 | Total loss: 0.00350854 | Loss_int: 0.00181333 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000012\n",
            "Epoch 5800 | Total loss: 0.00400477 | Loss_int: 0.00177000 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000014\n",
            "Epoch 5900 | Total loss: 0.01352888 | Loss_int: 0.00174904 | Loss_Clamped: 0.00000076 | Loss_dirichlet: 0.00000042\n",
            "Epoch 6000 | Total loss: 0.03091664 | Loss_int: 0.00174764 | Loss_Clamped: 0.00000136 | Loss_dirichlet: 0.00000155\n",
            "Epoch 6100 | Total loss: 0.00322309 | Loss_int: 0.00165028 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000011\n",
            "Epoch 6200 | Total loss: 0.02467604 | Loss_int: 0.00165851 | Loss_Clamped: 0.00000106 | Loss_dirichlet: 0.00000124\n",
            "Epoch 6300 | Total loss: 0.00349577 | Loss_int: 0.00157316 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000013\n",
            "Epoch 6400 | Total loss: 0.01611879 | Loss_int: 0.00156346 | Loss_Clamped: 0.00000068 | Loss_dirichlet: 0.00000078\n",
            "Epoch 6500 | Total loss: 0.00374642 | Loss_int: 0.00150981 | Loss_Clamped: 0.00000009 | Loss_dirichlet: 0.00000014\n",
            "Epoch 6600 | Total loss: 0.01034673 | Loss_int: 0.00149667 | Loss_Clamped: 0.00000056 | Loss_dirichlet: 0.00000033\n",
            "Epoch 6700 | Total loss: 0.00455853 | Loss_int: 0.00145178 | Loss_Clamped: 0.00000015 | Loss_dirichlet: 0.00000016\n",
            "Epoch 6800 | Total loss: 0.00315760 | Loss_int: 0.00141573 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000012\n",
            "Epoch 6900 | Total loss: 0.01923477 | Loss_int: 0.00143079 | Loss_Clamped: 0.00000096 | Loss_dirichlet: 0.00000082\n",
            "Epoch 7000 | Total loss: 0.00299811 | Loss_int: 0.00136195 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000011\n",
            "Epoch 7100 | Total loss: 0.00288395 | Loss_int: 0.00133998 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000011\n",
            "Epoch 7200 | Total loss: 0.01094574 | Loss_int: 0.00134128 | Loss_Clamped: 0.00000057 | Loss_dirichlet: 0.00000039\n",
            "Epoch 7300 | Total loss: 0.00500284 | Loss_int: 0.00129721 | Loss_Clamped: 0.00000020 | Loss_dirichlet: 0.00000017\n",
            "Epoch 7400 | Total loss: 0.05745938 | Loss_int: 0.00143305 | Loss_Clamped: 0.00000388 | Loss_dirichlet: 0.00000172\n",
            "Epoch 7500 | Total loss: 0.00266104 | Loss_int: 0.00124539 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000010\n",
            "Epoch 7600 | Total loss: 0.01161828 | Loss_int: 0.00125805 | Loss_Clamped: 0.00000062 | Loss_dirichlet: 0.00000041\n",
            "Epoch 7700 | Total loss: 0.03171245 | Loss_int: 0.00127730 | Loss_Clamped: 0.00000145 | Loss_dirichlet: 0.00000160\n",
            "Epoch 7800 | Total loss: 0.00256119 | Loss_int: 0.00118999 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000010\n",
            "Epoch 7900 | Total loss: 0.00271733 | Loss_int: 0.00117109 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000011\n",
            "Epoch 8000 | Total loss: 0.00327584 | Loss_int: 0.00115363 | Loss_Clamped: 0.00000009 | Loss_dirichlet: 0.00000012\n",
            "Epoch 8100 | Total loss: 0.00670060 | Loss_int: 0.00114475 | Loss_Clamped: 0.00000025 | Loss_dirichlet: 0.00000030\n",
            "Epoch 8200 | Total loss: 0.00667245 | Loss_int: 0.00113485 | Loss_Clamped: 0.00000020 | Loss_dirichlet: 0.00000036\n",
            "Epoch 8300 | Total loss: 0.08024998 | Loss_int: 0.00126349 | Loss_Clamped: 0.00000336 | Loss_dirichlet: 0.00000454\n",
            "Epoch 8400 | Total loss: 0.00281337 | Loss_int: 0.00109428 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000012\n",
            "Epoch 8500 | Total loss: 0.04923801 | Loss_int: 0.00126978 | Loss_Clamped: 0.00000185 | Loss_dirichlet: 0.00000294\n",
            "Epoch 8600 | Total loss: 0.00236723 | Loss_int: 0.00106318 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000009\n",
            "Epoch 8700 | Total loss: 0.00441729 | Loss_int: 0.00105663 | Loss_Clamped: 0.00000014 | Loss_dirichlet: 0.00000020\n",
            "Epoch 8800 | Total loss: 0.03114652 | Loss_int: 0.00110139 | Loss_Clamped: 0.00000113 | Loss_dirichlet: 0.00000188\n",
            "Epoch 8900 | Total loss: 0.00228166 | Loss_int: 0.00102943 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000009\n",
            "Epoch 9000 | Total loss: 0.00406426 | Loss_int: 0.00102179 | Loss_Clamped: 0.00000012 | Loss_dirichlet: 0.00000019\n",
            "Epoch 9100 | Total loss: 0.00615185 | Loss_int: 0.00101301 | Loss_Clamped: 0.00000031 | Loss_dirichlet: 0.00000021\n",
            "Epoch 9200 | Total loss: 0.00313727 | Loss_int: 0.00099453 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000014\n",
            "Epoch 9300 | Total loss: 0.00340257 | Loss_int: 0.00098396 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000016\n",
            "Epoch 9400 | Total loss: 0.00355682 | Loss_int: 0.00097510 | Loss_Clamped: 0.00000013 | Loss_dirichlet: 0.00000013\n",
            "Epoch 9500 | Total loss: 0.00566226 | Loss_int: 0.00096463 | Loss_Clamped: 0.00000020 | Loss_dirichlet: 0.00000027\n",
            "Epoch 9600 | Total loss: 0.00230297 | Loss_int: 0.00095104 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000009\n",
            "Epoch 9700 | Total loss: 0.01252927 | Loss_int: 0.00097049 | Loss_Clamped: 0.00000075 | Loss_dirichlet: 0.00000041\n",
            "Epoch 9800 | Total loss: 0.00221767 | Loss_int: 0.00093368 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000009\n",
            "Epoch 9900 | Total loss: 0.00366291 | Loss_int: 0.00092802 | Loss_Clamped: 0.00000010 | Loss_dirichlet: 0.00000017\n",
            "Epoch 10000 | Total loss: 0.00254917 | Loss_int: 0.00091268 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000011\n",
            "Epoch 10100 | Total loss: 0.00310282 | Loss_int: 0.00090931 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000014\n",
            "Epoch 10200 | Total loss: 0.00206344 | Loss_int: 0.00089926 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000009\n",
            "Epoch 10300 | Total loss: 0.00226185 | Loss_int: 0.00089178 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000009\n",
            "Epoch 10400 | Total loss: 0.01140411 | Loss_int: 0.00091654 | Loss_Clamped: 0.00000068 | Loss_dirichlet: 0.00000037\n",
            "Epoch 10500 | Total loss: 0.02168442 | Loss_int: 0.00093813 | Loss_Clamped: 0.00000135 | Loss_dirichlet: 0.00000072\n",
            "Epoch 10600 | Total loss: 0.00267095 | Loss_int: 0.00086905 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000012\n",
            "Epoch 10700 | Total loss: 0.00866392 | Loss_int: 0.00088186 | Loss_Clamped: 0.00000049 | Loss_dirichlet: 0.00000029\n",
            "Epoch 10800 | Total loss: 0.00262525 | Loss_int: 0.00085230 | Loss_Clamped: 0.00000007 | Loss_dirichlet: 0.00000011\n",
            "Epoch 10900 | Total loss: 0.00201227 | Loss_int: 0.00084927 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000009\n",
            "Epoch 11000 | Total loss: 0.00450653 | Loss_int: 0.00084688 | Loss_Clamped: 0.00000020 | Loss_dirichlet: 0.00000016\n",
            "Epoch 11100 | Total loss: 0.00214189 | Loss_int: 0.00082911 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000009\n",
            "Epoch 11200 | Total loss: 0.02619203 | Loss_int: 0.00091325 | Loss_Clamped: 0.00000083 | Loss_dirichlet: 0.00000170\n",
            "Epoch 11300 | Total loss: 0.00191634 | Loss_int: 0.00081689 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000008\n",
            "Epoch 11400 | Total loss: 0.00228336 | Loss_int: 0.00081122 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000010\n",
            "Epoch 11500 | Total loss: 0.00186107 | Loss_int: 0.00080330 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000008\n",
            "Epoch 11600 | Total loss: 0.02231923 | Loss_int: 0.00086121 | Loss_Clamped: 0.00000082 | Loss_dirichlet: 0.00000133\n",
            "Epoch 11700 | Total loss: 0.00272843 | Loss_int: 0.00079656 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000012\n",
            "Epoch 11800 | Total loss: 0.00228740 | Loss_int: 0.00078856 | Loss_Clamped: 0.00000005 | Loss_dirichlet: 0.00000010\n",
            "Epoch 11900 | Total loss: 0.03947050 | Loss_int: 0.00086950 | Loss_Clamped: 0.00000230 | Loss_dirichlet: 0.00000156\n",
            "Epoch 12000 | Total loss: 0.00249333 | Loss_int: 0.00077589 | Loss_Clamped: 0.00000006 | Loss_dirichlet: 0.00000011\n",
            "Epoch 12100 | Total loss: 0.03032899 | Loss_int: 0.00086723 | Loss_Clamped: 0.00000103 | Loss_dirichlet: 0.00000191\n",
            "Epoch 12200 | Total loss: 0.00342604 | Loss_int: 0.00076683 | Loss_Clamped: 0.00000009 | Loss_dirichlet: 0.00000017\n",
            "Epoch 12300 | Total loss: 0.00364508 | Loss_int: 0.00076714 | Loss_Clamped: 0.00000012 | Loss_dirichlet: 0.00000017\n",
            "Epoch 12400 | Total loss: 0.00186254 | Loss_int: 0.00075637 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000008\n",
            "Epoch 12500 | Total loss: 0.00176514 | Loss_int: 0.00075159 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000008\n",
            "Epoch 12600 | Total loss: 0.00185798 | Loss_int: 0.00074220 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000008\n",
            "Epoch 12700 | Total loss: 0.00276266 | Loss_int: 0.00074557 | Loss_Clamped: 0.00000008 | Loss_dirichlet: 0.00000012\n",
            "Epoch 12800 | Total loss: 0.00534339 | Loss_int: 0.00074231 | Loss_Clamped: 0.00000028 | Loss_dirichlet: 0.00000018\n",
            "Epoch 12900 | Total loss: 0.00411436 | Loss_int: 0.00073523 | Loss_Clamped: 0.00000019 | Loss_dirichlet: 0.00000015\n",
            "Epoch 13000 | Total loss: 0.00199914 | Loss_int: 0.00072078 | Loss_Clamped: 0.00000004 | Loss_dirichlet: 0.00000009\n",
            "Epoch 13100 | Total loss: 0.03417292 | Loss_int: 0.00081313 | Loss_Clamped: 0.00000192 | Loss_dirichlet: 0.00000142\n",
            "Epoch 13200 | Total loss: 0.00169358 | Loss_int: 0.00071087 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000008\n",
            "Epoch 13300 | Total loss: 0.01581730 | Loss_int: 0.00074209 | Loss_Clamped: 0.00000089 | Loss_dirichlet: 0.00000061\n",
            "Epoch 13400 | Total loss: 0.01411140 | Loss_int: 0.00074804 | Loss_Clamped: 0.00000048 | Loss_dirichlet: 0.00000086\n",
            "Epoch 13500 | Total loss: 0.00714626 | Loss_int: 0.00071420 | Loss_Clamped: 0.00000040 | Loss_dirichlet: 0.00000025\n",
            "Epoch 13600 | Total loss: 0.00294478 | Loss_int: 0.00069786 | Loss_Clamped: 0.00000011 | Loss_dirichlet: 0.00000011\n",
            "Epoch 13700 | Total loss: 0.00168262 | Loss_int: 0.00068620 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000008\n",
            "Epoch 13800 | Total loss: 0.02035713 | Loss_int: 0.00073030 | Loss_Clamped: 0.00000091 | Loss_dirichlet: 0.00000105\n",
            "Epoch 13900 | Total loss: 0.01029763 | Loss_int: 0.00070430 | Loss_Clamped: 0.00000038 | Loss_dirichlet: 0.00000058\n",
            "Epoch 14000 | Total loss: 0.00159661 | Loss_int: 0.00067016 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000007\n",
            "Epoch 14100 | Total loss: 0.00159353 | Loss_int: 0.00066713 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000007\n",
            "Epoch 14200 | Total loss: 0.03226791 | Loss_int: 0.00077816 | Loss_Clamped: 0.00000139 | Loss_dirichlet: 0.00000176\n",
            "Epoch 14300 | Total loss: 0.00174265 | Loss_int: 0.00065690 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000008\n",
            "Epoch 14400 | Total loss: 0.00443634 | Loss_int: 0.00067726 | Loss_Clamped: 0.00000012 | Loss_dirichlet: 0.00000025\n",
            "Epoch 14500 | Total loss: 0.00652211 | Loss_int: 0.00066296 | Loss_Clamped: 0.00000037 | Loss_dirichlet: 0.00000022\n",
            "Epoch 14600 | Total loss: 0.00293404 | Loss_int: 0.00064959 | Loss_Clamped: 0.00000011 | Loss_dirichlet: 0.00000011\n",
            "Epoch 14700 | Total loss: 0.00165981 | Loss_int: 0.00063995 | Loss_Clamped: 0.00000003 | Loss_dirichlet: 0.00000008\n",
            "Epoch 14800 | Total loss: 0.00154251 | Loss_int: 0.00063396 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000007\n",
            "Epoch 14900 | Total loss: 0.03755387 | Loss_int: 0.00075053 | Loss_Clamped: 0.00000208 | Loss_dirichlet: 0.00000160\n",
            "Epoch 15000 | Total loss: 0.00458024 | Loss_int: 0.00063543 | Loss_Clamped: 0.00000015 | Loss_dirichlet: 0.00000025\n",
            "Epoch 15100 | Total loss: 0.00157973 | Loss_int: 0.00062120 | Loss_Clamped: 0.00000002 | Loss_dirichlet: 0.00000007\n",
            "Epoch 15200 | Total loss: 0.00764294 | Loss_int: 0.00062875 | Loss_Clamped: 0.00000037 | Loss_dirichlet: 0.00000033\n"
          ]
        }
      ]
    }
  ]
}