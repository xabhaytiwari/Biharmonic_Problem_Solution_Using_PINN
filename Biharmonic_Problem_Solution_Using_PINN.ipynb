{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xabhaytiwari/Biharmonic_Problem_Solution_Using_PINN/blob/main/Biharmonic_Problem_Solution_Using_PINN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p7Z-xM1K8S2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class NN(torch.nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super(NN, self).__init__()\n",
        "    self.L1 = torch.nn.Linear(2, 60) # Wider\n",
        "    self.L2 = torch.nn.Linear(60, 60)\n",
        "    self.L3 = torch.nn.Linear(60, 60)\n",
        "    self.L4 = torch.nn.Linear(60, 60)\n",
        "    self.L5 = torch.nn.Linear(60, 60) # Deeper\n",
        "    self.L6 = torch.nn.Linear(60, 1)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    inputs = torch.cat([x, y], axis=1)\n",
        "    x1 = torch.tanh(self.L1(inputs))\n",
        "    x2 = torch.tanh(self.L2(x1))\n",
        "    x3 = torch.tanh(self.L3(x2))\n",
        "    x4 = torch.tanh(self.L4(x3))\n",
        "    x5 = torch.tanh(self.L5(x4))\n",
        "    x6 = self.L6(x5)\n",
        "    return x6\n",
        "\n",
        "  def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll gather Inputs, Collocation points, PDE Data and Balancing Parameter alpha.\n"
      ],
      "metadata": {
        "id": "Ks7LqF3FM7PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff, sin, pi, lambdify\n",
        "\n",
        "x1, x2 = symbols('x1 x2')\n",
        "u = (1 / (2 * (pi ** 2))) * sin(pi * x1) * sin(pi * x2)\n",
        "# u =\n",
        "\n",
        "laplacian_u = diff(u, x1, 2) + diff(u, x2, 2)\n",
        "bi_laplacian_u = diff(laplacian_u, x1, 2) + diff(laplacian_u, x2, 2)\n",
        "\n",
        "f_sym = bi_laplacian_u\n",
        "\n",
        "u_x_sym = diff(u, x1)\n",
        "u_y_sym = diff(u, x2)\n",
        "\n",
        "u_xx_sym = diff(u, x1, 2)\n",
        "u_yy_sym = diff(u, x2, 2)\n",
        "u_xy_sym = diff(u, x1, x2)\n",
        "\n",
        "ldu = lambdify((x1, x2), u, 'numpy')\n",
        "ldu_x = lambdify((x1, x2), u_x_sym, 'numpy')\n",
        "ldu_y = lambdify((x1, x2), u_y_sym, 'numpy')\n",
        "ldf = lambdify((x1, x2), f_sym, 'numpy')\n",
        "\n",
        "ldu_xx = lambdify((x1, x2), u_xx_sym, 'numpy')\n",
        "ldu_yy = lambdify((x1, x2), u_yy_sym, 'numpy')\n",
        "ldu_xy = lambdify((x1, x2), u_xy_sym, 'numpy')\n",
        "\n",
        "def from_seq_to_array(items):\n",
        "    out = []\n",
        "    for item in items:\n",
        "        out.append(np.array(item).reshape(-1, 1))\n",
        "    if len(out) == 1:\n",
        "        return out[0]\n",
        "    return out\n",
        "\n",
        "\n",
        "def data_gen_interior(collocations):\n",
        "    u_gt = [ldu(x, y) for x, y in collocations]\n",
        "    f_gt = [ldf(x, y) for x, y in collocations]\n",
        "    return from_seq_to_array([u_gt, f_gt])\n",
        "\n",
        "def data_gen_bdry(collocations, normal_vec):\n",
        "    ybdry_vals_g1       = []\n",
        "    ybdry_vals_g2       = []\n",
        "\n",
        "    for (x, y), (n1, n2) in zip(collocations, normal_vec):\n",
        "        ybdry_vals_g1.append(ldu(x, y))\n",
        "        ybdry_vals_g2.append(ldu_x(x, y) * n1 + ldu_y(x, y) * n2)\n",
        "\n",
        "    return from_seq_to_array([ybdry_vals_g1, ybdry_vals_g2])"
      ],
      "metadata": {
        "id": "K3xFxKlXNmDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "from scipy.stats import uniform\n",
        "import os\n",
        "\n",
        "N = 20000\n",
        "dataname = '20000pts'\n",
        "\n",
        "domain_data_x = uniform.rvs(size=N)\n",
        "domain_data_y = uniform.rvs(size=N)\n",
        "\n",
        "domain_data = np.array([domain_data_x,domain_data_y]).T\n",
        "print(domain_data.shape)\n",
        "\n",
        "\n",
        "Nb = 10000\n",
        "\n",
        "def generate_random_bdry(Nb):\n",
        "    '''\n",
        "    Generate random boundary points.\n",
        "    '''\n",
        "    bdry_col = uniform.rvs(size=Nb*2).reshape([Nb,2])\n",
        "    for i in range(Nb):\n",
        "        randind = np.random.randint(0,2)\n",
        "        if bdry_col[i,randind] <= 0.5:\n",
        "            bdry_col[i,randind] = 0.0\n",
        "        else:\n",
        "            bdry_col[i,randind] = 1.0\n",
        "\n",
        "    return bdry_col\n",
        "\n",
        "\n",
        "def compute_normals(bdry_col, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Given bdry_col of shape (Nb,2) with points on the edges of [0,1]^2,\n",
        "    returns two arrays n1,n2 of shape (Nb,1) giving the outward unit normals.\n",
        "\n",
        "    Assumes:\n",
        "      - if x ≈ 0 → normal = (-1, 0)\n",
        "      - if x ≈ 1 → normal = ( 1, 0)\n",
        "      - if y ≈ 0 → normal = ( 0,-1)\n",
        "      - if y ≈ 1 → normal = ( 0, 1)\n",
        "    \"\"\"\n",
        "    x = bdry_col[:, 0]\n",
        "    y = bdry_col[:, 1]\n",
        "\n",
        "    n1 = np.zeros_like(x)\n",
        "    n2 = np.zeros_like(y)\n",
        "\n",
        "    # left edge x=0\n",
        "    mask = np.isclose(x, 0.0, atol=eps)\n",
        "    n1[mask] = -1.0;  n2[mask] =  0.0\n",
        "\n",
        "    # right edge x=1\n",
        "    mask = np.isclose(x, 1.0, atol=eps)\n",
        "    n1[mask] =  1.0;  n2[mask] =  0.0\n",
        "\n",
        "    # bottom edge y=0\n",
        "    mask = np.isclose(y, 0.0, atol=eps)\n",
        "    n1[mask] =  0.0;  n2[mask] = -1.0\n",
        "\n",
        "    # top edge y=1\n",
        "    mask = np.isclose(y, 1.0, atol=eps)\n",
        "    n1[mask] =  0.0;  n2[mask] =  1.0\n",
        "\n",
        "    # reshape to column vectors\n",
        "    return n1.reshape(-1,1), n2.reshape(-1,1)\n",
        "\n",
        "bdry_col = generate_random_bdry(Nb)\n",
        "n1_np, n2_np = compute_normals(bdry_col)\n",
        "normal_vec = np.hstack([n1_np, n2_np])\n",
        "\n",
        "print(normal_vec.shape)\n",
        "print(bdry_col.shape)\n",
        "\n",
        "if not os.path.exists('dataset/'):\n",
        "    os.makedirs('dataset/')\n",
        "with open('dataset/'+dataname,'wb') as pfile:\n",
        "    pkl.dump(domain_data,pfile)\n",
        "    pkl.dump(bdry_col,pfile)\n",
        "    pkl.dump(normal_vec,pfile)\n",
        "\n",
        "\n",
        "ugt,fgt = data_gen_interior(domain_data)\n",
        "dirichlet_data, clamped_data = data_gen_bdry(bdry_col,normal_vec)\n",
        "\n",
        "\n",
        "with open(\"dataset/gt_on_{}\".format(dataname),'wb') as pfile:\n",
        "    pkl.dump(ugt,pfile)\n",
        "    pkl.dump(fgt,pfile)\n",
        "    pkl.dump(dirichlet_data,pfile)\n",
        "    pkl.dump(clamped_data,pfile)"
      ],
      "metadata": {
        "id": "_C8FJwO4NYEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf24c82-eb97-4d01-cf79-b3af6f8330ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 2)\n",
            "(10000, 2)\n",
            "(10000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "def pde(x1, x2, net):\n",
        "    \"\"\"\n",
        "    Compute Δ²u = (u_xx + u_yy)_xx + (u_xx + u_yy)_yy\n",
        "    at the points (x1, x2) via automatic differentiation.\n",
        "    \"\"\"\n",
        "    # 1) forward pass\n",
        "    u = net(x1, x2)\n",
        "\n",
        "    # 2) first derivatives\n",
        "    # u_x, u_y = torch.autograd.grad(\n",
        "    #     u.sum(), (x1, x2), create_graph=True\n",
        "    # )\n",
        "    u_x = torch.autograd.grad(u.sum(), x1, create_graph=True)[0]\n",
        "    u_y = torch.autograd.grad(u.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "    # 3) second derivatives\n",
        "    u_xx = torch.autograd.grad(u_x.sum(), x1, create_graph=True)[0]\n",
        "    u_yy = torch.autograd.grad(u_y.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "    # 4) Laplacian\n",
        "    lap = u_xx + u_yy\n",
        "\n",
        "    # 5) third derivatives of lap\n",
        "    # lap_x, lap_y = torch.autograd.grad(\n",
        "    #     lap.sum(), (x1, x2), create_graph=True\n",
        "    # )\n",
        "    lap_x = torch.autograd.grad(lap.sum(), x1, create_graph=True)[0]\n",
        "    lap_y = torch.autograd.grad(lap.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "    # 6) fourth derivatives (bi‐Laplacian pieces)\n",
        "    lap_xx = torch.autograd.grad(lap_x.sum(), x1, create_graph=True)[0]\n",
        "    lap_yy = torch.autograd.grad(lap_y.sum(), x2, create_graph=True)[0]\n",
        "\n",
        "\n",
        "\n",
        "    return lap_xx +  lap_yy\n",
        "\n",
        "\n",
        "\n",
        "def bdry(x1,x2,n1,n2,net):\n",
        "    out = net(x1,x2)\n",
        "    u_x = torch.autograd.grad(out.sum(),x1,create_graph=True)[0]\n",
        "    u_y = torch.autograd.grad(out.sum(),x2,create_graph=True)[0]\n",
        "\n",
        "    return out, u_x*n1 + u_y*n2\n",
        "\n",
        "\n",
        "def pdeloss(net,intx1,intx2,pdedata,bdx1,bdx2,nx1,nx2,bdrydata_diri,bdrydat_clamped,bw_diri,bw_clamped):\n",
        "    bdx1  = bdx1.requires_grad_(True)\n",
        "    bdx2  = bdx2.requires_grad_(True)\n",
        "    pout = pde(intx1,intx2,net)\n",
        "    bout_diri, bout_clamped = bdry(bdx1,bdx2,nx1,nx2,net)\n",
        "    pres = mse_loss(pout,pdedata)\n",
        "    bres = bw_diri*mse_loss(bout_diri,bdrydata_diri) + bw_clamped*mse_loss(bout_clamped,bdrydat_clamped)\n",
        "    diri_loss = mse_loss(bout_diri,bdrydata_diri)\n",
        "    clamped_loss = mse_loss(bout_clamped,bdrydat_clamped)\n",
        "\n",
        "\n",
        "    loss = pres + bres\n",
        "\n",
        "    return loss, pres, bres, diri_loss, clamped_loss"
      ],
      "metadata": {
        "id": "kogMf2FV4btP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def from_numpy_to_tensor(numpys,require_grads,dtype=torch.float32):\n",
        "    outputs = list()\n",
        "    for ind in range(len(numpys)):\n",
        "        outputs.append(\n",
        "            Variable(torch.from_numpy(numpys[ind]),requires_grad=require_grads[ind]).type(dtype)\n",
        "        )\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "uwukoYQM8Vob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from tracemalloc import start\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.optim as opt\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import torch.utils.data.dataloader as Dataloader\n",
        "from torch.autograd import Variable\n",
        "import pickle as pkl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "from time import time\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "torch.set_default_dtype(torch.float32)\n",
        "start_time = time()\n",
        "\n",
        "y = NN()\n",
        "y.apply(NN.init_weights)\n",
        "y.to(device)\n",
        "\n",
        "dataname = '20000pts'\n",
        "name = 'results/'\n",
        "bw_dir = 1e4   # Start with 10,000\n",
        "bw_clamp = 1e4 # Start with 10,000\n",
        "\n",
        "if not os.path.exists(name):\n",
        "    os.makedirs(name)\n",
        "if not os.path.exists(name+\"y_plot/\"):\n",
        "    os.makedirs(name+\"y_plot/\")\n",
        "\n",
        "params = list(y.parameters())\n",
        "\n",
        "# --- 1. Load all data from files ---\n",
        "with open(\"dataset/\"+dataname,'rb') as pfile:\n",
        "    int_col = pkl.load(pfile)\n",
        "    bdry_col = pkl.load(pfile)\n",
        "    normal_vec = pkl.load(pfile)\n",
        "\n",
        "with open(\"dataset/gt_on_{}\".format(dataname),'rb') as pfile:\n",
        "    u_gt_np = pkl.load(pfile)\n",
        "    f_np = pkl.load(pfile)\n",
        "    dirichlet_data_np = pkl.load(pfile)\n",
        "    clamped_data_np = pkl.load(pfile)\n",
        "\n",
        "print(f\"Interior points: {int_col.shape}\")\n",
        "print(f\"Boundary points: {bdry_col.shape}\")\n",
        "print(f\"Forcing term: {f_np.shape}\")\n",
        "print(f\"Dirichlet data: {dirichlet_data_np.shape}\")\n",
        "print(f\"Clamped data: {clamped_data_np.shape}\")\n",
        "\n",
        "def compute_derivatives(net, x, y):\n",
        "    \"\"\" Computes u, 1st, and 2nd derivatives \"\"\"\n",
        "    u = net(x, y)\n",
        "\n",
        "    # 1st derivatives\n",
        "    u_x, u_y = torch.autograd.grad(\n",
        "        u.sum(), (x, y), create_graph=True\n",
        "    )\n",
        "\n",
        "    # 2nd derivatives\n",
        "    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
        "    u_yy = torch.autograd.grad(u_y.sum(), y, create_graph=True)[0]\n",
        "    u_xy = torch.autograd.grad(u_x.sum(), y, create_graph=True)[0]\n",
        "\n",
        "    return u, u_x, u_y, u_xx, u_yy, u_xy\n",
        "\n",
        "# --- 2. Create Tensors and DataLoaders ---\n",
        "\n",
        "# Create a TensorDataset for INTERIOR points AND their forcing term ---\n",
        "intx1_np, intx2_np = np.split(int_col, 2, axis=1)\n",
        "\n",
        "# Convert interior data to tensors (no grad needed here, will be set in loop)\n",
        "tintx1 = torch.from_numpy(intx1_np).float()\n",
        "tintx2 = torch.from_numpy(intx2_np).float()\n",
        "f_tensor = torch.from_numpy(f_np).float()\n",
        "\n",
        "# Create a dataset and loader for interior points\n",
        "interior_dataset = TensorDataset(tintx1, tintx2, f_tensor)\n",
        "loader = torch.utils.data.DataLoader(interior_dataset, batch_size=2000, shuffle=True)\n",
        "\n",
        "\n",
        "# Create Tensors for BOUNDARY data (with requires_grad=True) ---\n",
        "bdx1_np, bdx2_np = np.split(bdry_col, 2, axis=1)\n",
        "nx1_np, nx2_np = np.split(normal_vec, 2, axis=1)\n",
        "\n",
        "# Boundary coordinates MUST have requires_grad=True\n",
        "# Normals and ground truth data do NOT need gradients.\n",
        "[tbdx1, tbdx2] = from_numpy_to_tensor([bdx1_np, bdx2_np], [True, True], dtype=torch.float32)\n",
        "[tnx1, tnx2] = from_numpy_to_tensor([nx1_np, nx2_np], [False, False], dtype=torch.float32)\n",
        "[bdrydata_dirichlet, bdrydata_clamped] = from_numpy_to_tensor(\n",
        "    [dirichlet_data_np, clamped_data_np], [False, False], dtype=torch.float32\n",
        ")\n",
        "\n",
        "# Move all *full* boundary tensors to the device once\n",
        "tbdx1 = tbdx1.to(device)\n",
        "tbdx2 = tbdx2.to(device)\n",
        "tnx1 = tnx1.to(device)\n",
        "tnx2 = tnx2.to(device)\n",
        "bdrydata_dirichlet = bdrydata_dirichlet.to(device)\n",
        "bdrydata_clamped = bdrydata_clamped.to(device)\n",
        "\n",
        "# Note: We won't use ygt for training, only for final error calculation\n",
        "ygt = torch.from_numpy(u_gt_np).float().to(device)\n",
        "\n",
        "optimizer = opt.Adam(params, lr=1e-4)\n",
        "scheduler = opt.lr_scheduler.ReduceLROnPlateau(optimizer, patience=500)\n",
        "train_start_time = time()\n",
        "# --- 3. Define the Closure ---\n",
        "def closure():\n",
        "    tot_loss = 0\n",
        "    tot_pres = 0\n",
        "    tot_diri_loss = 0\n",
        "    tot_clamped_loss = 0\n",
        "\n",
        "    # Iterate over the DataLoader\n",
        "    for i, (intx1_batch, intx2_batch, f_batch) in enumerate(loader):\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       # Set requires_grad=True for the BATCH coordinates\n",
        "       ttintx1 = intx1_batch.to(device).requires_grad_(True)\n",
        "       ttintx2 = intx2_batch.to(device).requires_grad_(True)\n",
        "\n",
        "       # This is the ground-truth forcing term for the batch\n",
        "       f_gt_batch = f_batch.to(device)\n",
        "\n",
        "       # --- Pass the correct batch data to the loss function ---\n",
        "       # We use the *batched* interior points and *batched* forcing term\n",
        "       # We use the *full* set of boundary points\n",
        "       loss, pres, bres, diri_loss, clamped_loss = pdeloss(\n",
        "           y,\n",
        "           ttintx1, ttintx2, f_gt_batch,  # Batched interior data\n",
        "           tbdx1, tbdx2, tnx1, tnx2,     # Full boundary data\n",
        "           bdrydata_dirichlet, bdrydata_clamped,\n",
        "           bw_dir, bw_clamp\n",
        "       )\n",
        "\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       # Detach losses to free memory\n",
        "       tot_loss += loss.detach()\n",
        "       tot_pres += pres.detach()\n",
        "       tot_diri_loss += diri_loss.detach()\n",
        "       tot_clamped_loss += clamped_loss.detach()\n",
        "\n",
        "    # Get the average loss per batch\n",
        "    num_batches = len(loader)\n",
        "    avg_loss = tot_loss / num_batches\n",
        "    avg_pres = tot_pres / num_batches\n",
        "    avg_diri = tot_diri_loss / num_batches\n",
        "    avg_clamp = tot_clamped_loss / num_batches\n",
        "\n",
        "    # scheduler.step() uses the average loss\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    return (\n",
        "        avg_loss.cpu().numpy(),\n",
        "        avg_pres.cpu().numpy(),\n",
        "        avg_diri.cpu().numpy(),\n",
        "        avg_clamp.cpu().numpy()\n",
        "    )\n",
        "\n",
        "# --- 4. Run the Training Loop ---\n",
        "losslist = list()\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(20000):\n",
        "    loss, presloss, diri_loss, clamped_loss = closure()\n",
        "    losslist.append(loss)\n",
        "    if epoch % 100 == 0:\n",
        "         print(f\"Epoch {epoch} | Total loss: {loss:.8f} | Loss_int: {presloss:.8f} | Loss_Clamped: {clamped_loss:.8f} | Loss_dirichlet: {diri_loss:.8f}\")\n",
        "train_end_time = time()\n",
        "print(\"Training finished.\")\n",
        "with open(\"results/loss.pkl\",'wb') as pfile:\n",
        "    pkl.dump(losslist,pfile)\n",
        "\n",
        "# --- (After training is finished) ---\n",
        "\n",
        "print(\"Calculating final errors...\")\n",
        "\n",
        "# Use the same grid as your plots\n",
        "x_pts = np.linspace(0,1,200)\n",
        "y_pts = np.linspace(0,1,200)\n",
        "ms_x, ms_y = np.meshgrid(x_pts,y_pts)\n",
        "x_flat = np.ravel(ms_x).reshape(-1,1)\n",
        "y_flat = np.ravel(ms_y).reshape(-1,1)\n",
        "collocations = np.concatenate([x_flat, y_flat], axis=1)\n",
        "\n",
        "# --- 1. Get Ground Truth (GT) values ---\n",
        "u_gt_np = ldu(x_flat, y_flat).flatten()\n",
        "ux_gt_np = ldu_x(x_flat, y_flat).flatten()\n",
        "uy_gt_np = ldu_y(x_flat, y_flat).flatten()\n",
        "uxx_gt_np = ldu_xx(x_flat, y_flat).flatten()\n",
        "uyy_gt_np = ldu_yy(x_flat, y_flat).flatten()\n",
        "uxy_gt_np = ldu_xy(x_flat, y_flat).flatten()\n",
        "\n",
        "# --- 2. Get Network Prediction (NN) values ---\n",
        "pt_x = torch.from_numpy(x_flat).float().to(device).requires_grad_(True)\n",
        "pt_y = torch.from_numpy(y_flat).float().to(device).requires_grad_(True)\n",
        "\n",
        "u_nn, ux_nn, uy_nn, uxx_nn, uyy_nn, uxy_nn = compute_derivatives(y, pt_x, pt_y)\n",
        "\n",
        "u_nn_np = u_nn.data.cpu().numpy().flatten()\n",
        "ux_nn_np = ux_nn.data.cpu().numpy().flatten()\n",
        "uy_nn_np = uy_nn.data.cpu().numpy().flatten()\n",
        "uxx_nn_np = uxx_nn.data.cpu().numpy().flatten()\n",
        "uyy_nn_np = uyy_nn.data.cpu().numpy().flatten()\n",
        "uxy_nn_np = uxy_nn.data.cpu().numpy().flatten()\n",
        "\n",
        "# --- 3. Compute Errors ---\n",
        "# L2 Error\n",
        "l2_error_u = np.sqrt(np.mean((u_gt_np - u_nn_np)**2))\n",
        "l2_error = l2_error_u / np.sqrt(np.mean(u_gt_np**2))\n",
        "\n",
        "# H1 Error\n",
        "l2_error_ux = np.sqrt(np.mean((ux_gt_np - ux_nn_np)**2))\n",
        "l2_error_uy = np.sqrt(np.mean((uy_gt_np - uy_nn_np)**2))\n",
        "h1_error = np.sqrt(l2_error_u**2 + l2_error_ux**2 + l2_error_uy**2) / \\\n",
        "           np.sqrt(np.mean(u_gt_np**2) + np.mean(ux_gt_np**2) + np.mean(uy_gt_np**2))\n",
        "\n",
        "# H2 Error\n",
        "l2_error_uxx = np.sqrt(np.mean((uxx_gt_np - uxx_nn_np)**2))\n",
        "l2_error_uyy = np.sqrt(np.mean((uyy_gt_np - uyy_nn_np)**2))\n",
        "l2_error_uxy = np.sqrt(np.mean((uxy_gt_np - uxy_nn_np)**2))\n",
        "h2_error = np.sqrt(l2_error_u**2 + l2_error_ux**2 + l2_error_uy**2 + \\\n",
        "                   l2_error_uxx**2 + l2_error_uyy**2 + l2_error_uxy**2) / \\\n",
        "           np.sqrt(np.mean(u_gt_np**2) + np.mean(ux_gt_np**2) + np.mean(uy_gt_np**2) + \\\n",
        "                   np.mean(uxx_gt_np**2) + np.mean(uyy_gt_np**2) + np.mean(uxy_gt_np**2))\n",
        "\n",
        "print(f\"Relative L2 Error: {l2_error:.4e}\")\n",
        "print(f\"Relative H1 Error: {h1_error:.4e}\")\n",
        "print(f\"Relative H2 Error: {h2_error:.4e}\")\n",
        "\n",
        "# --- 4. Reshape for Plotting ---\n",
        "ms_ysol = u_nn_np.reshape(ms_x.shape)\n",
        "ms_ugt = u_gt_np.reshape(ms_x.shape)\n",
        "\n",
        "# --- Plot Loss History ---\n",
        "print(\"Plotting loss history...\")\n",
        "fig_loss = plt.figure(figsize=(10, 6))\n",
        "plt.plot(losslist)\n",
        "plt.yscale('log') # Log scale is essential for loss plots\n",
        "plt.title('Training Loss Dynamics')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.savefig(name + 'loss_history.png')\n",
        "plt.close(fig_loss)\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "print(\"\\n---Training Summary ---\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"\\n[Network Architecture]\")\n",
        "print(y)\n",
        "print(\"\\n[Hyperparameters]\")\n",
        "print(f\"  Optimizer: Adam (lr={1e-4})\")\n",
        "print(f\"  Epochs (Adam): {20000}\")\n",
        "print(f\"  Batch Size: {2000}\")\n",
        "print(f\"  Interior Points: {int_col.shape[0]}\")\n",
        "print(f\"  Boundary Points: {bdry_col.shape[0]}\")\n",
        "print(f\"  Loss Weight (Dirichlet): {bw_dir:.1e}\")\n",
        "print(f\"  Loss Weight (Clamped): {bw_clamp:.1e}\")\n",
        "print(\"\\n[Performance]\")\n",
        "print(f\"  Training Time: {train_end_time - train_start_time:.2f} seconds\")\n",
        "print(f\"  Total Script Time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"  Final Total Loss: {losslist[-1]:.4e}\")\n",
        "print(f\"  Relative L2 Error: {l2_error:.4e}\")\n",
        "print(f\"  Relative H1 Error: {h1_error:.4e}\")\n",
        "print(f\"  Relative H2 Error: {h2_error:.4e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VNK2GLV70bN",
        "outputId": "338fed2e-ad04-4e27-cdec-bd43a1535bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Interior points: (20000, 2)\n",
            "Boundary points: (10000, 2)\n",
            "Forcing term: (20000, 1)\n",
            "Dirichlet data: (10000, 1)\n",
            "Clamped data: (10000, 1)\n",
            "Starting training...\n",
            "Epoch 0 | Total loss: 665.52972412 | Loss_int: 109.56639862 | Loss_Clamped: 0.03863931 | Loss_dirichlet: 0.01695701\n",
            "Epoch 100 | Total loss: 22.33473778 | Loss_int: 11.62869549 | Loss_Clamped: 0.00097238 | Loss_dirichlet: 0.00009822\n",
            "Epoch 200 | Total loss: 10.35236645 | Loss_int: 5.89986324 | Loss_Clamped: 0.00039094 | Loss_dirichlet: 0.00005431\n",
            "Epoch 300 | Total loss: 1.47241628 | Loss_int: 1.18992841 | Loss_Clamped: 0.00002211 | Loss_dirichlet: 0.00000614\n",
            "Epoch 400 | Total loss: 0.12336168 | Loss_int: 0.08936986 | Loss_Clamped: 0.00000234 | Loss_dirichlet: 0.00000106\n",
            "Epoch 500 | Total loss: 0.05466552 | Loss_int: 0.04322579 | Loss_Clamped: 0.00000088 | Loss_dirichlet: 0.00000027\n",
            "Epoch 600 | Total loss: 0.04786317 | Loss_int: 0.03940681 | Loss_Clamped: 0.00000054 | Loss_dirichlet: 0.00000030\n",
            "Epoch 700 | Total loss: 0.04174147 | Loss_int: 0.03538398 | Loss_Clamped: 0.00000036 | Loss_dirichlet: 0.00000028\n",
            "Epoch 800 | Total loss: 0.03533702 | Loss_int: 0.03151895 | Loss_Clamped: 0.00000020 | Loss_dirichlet: 0.00000018\n",
            "Epoch 900 | Total loss: 0.04879283 | Loss_int: 0.02807700 | Loss_Clamped: 0.00000105 | Loss_dirichlet: 0.00000102\n",
            "Epoch 1000 | Total loss: 0.11928463 | Loss_int: 0.02526925 | Loss_Clamped: 0.00000321 | Loss_dirichlet: 0.00000619\n",
            "Epoch 1100 | Total loss: 0.04109235 | Loss_int: 0.02270546 | Loss_Clamped: 0.00000091 | Loss_dirichlet: 0.00000093\n",
            "Epoch 1200 | Total loss: 0.04171639 | Loss_int: 0.02045380 | Loss_Clamped: 0.00000099 | Loss_dirichlet: 0.00000114\n",
            "Epoch 1300 | Total loss: 0.04775122 | Loss_int: 0.01841944 | Loss_Clamped: 0.00000200 | Loss_dirichlet: 0.00000093\n",
            "Epoch 1400 | Total loss: 0.01923538 | Loss_int: 0.01652388 | Loss_Clamped: 0.00000010 | Loss_dirichlet: 0.00000017\n",
            "Epoch 1500 | Total loss: 0.05783634 | Loss_int: 0.01492635 | Loss_Clamped: 0.00000295 | Loss_dirichlet: 0.00000135\n"
          ]
        }
      ]
    }
  ]
}